When writing about neural networks, many authors (most of them in fact) start by describing the single-layer perceptron, talking about its restrictions (XOR problem), and then introduce the awesome multi-layer perceptron with backpropagation learning procedure and its amazing capabilities. Many famous software libraries share this terminology. For example, scikit-learn with its MLPClassifier (Multi-Layer Perceptron Classifier).
But in his Neural Networks for Machine Learning course on Coursera Jeoffrey Hinton argues that we shouldn’t use such term as “multi-layer perceptron”, because the perceptron learning procedure can not be applied to hidden layers. 
Personally, I just think that the term “perceptron” is even more misleading than “neural network”. Very few people can clearly define what perceptron is, and most of us mean different things when speaking of perceptrons. So I prefer to apply the term “perceptron” only to those neural networks which adjust their weights using the perceptron learning procedure (which is a simple error-correction rule that can not be applied to multiple layers).
